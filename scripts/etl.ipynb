{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60893096",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hola Spark desde VS Code ðŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2c1b2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‰ JDBC URL: jdbc:sqlserver://host.docker.internal:1433;databaseName=olva;encrypt=false;trustServerCertificate=true\n",
      "ðŸ‘‰ Usuario: etl_user\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/scripts/config\")\n",
    "from db_config import db_config\n",
    "\n",
    "print(\"ðŸ‘‰ JDBC URL:\", db_config[\"jdbc_url\"])\n",
    "print(\"ðŸ‘‰ Usuario:\", db_config[\"user\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20700c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DiagnÃ³stico rÃ¡pido de entorno/red/Spark =====\n",
    "import os, sys, socket, platform\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"SO:\", platform.platform())\n",
    "print(\"HOSTNAME:\", socket.gethostname())\n",
    "print(\"SPARK_HOME:\", os.environ.get(\"SPARK_HOME\"))\n",
    "\n",
    "# Â¿EstÃ¡ pyspark instalado y quÃ© versiÃ³n?\n",
    "try:\n",
    "    import pyspark\n",
    "    print(\"pyspark.__version__:\", pyspark.__version__)\n",
    "except Exception as e:\n",
    "    print(\"pyspark import ERROR:\", e)\n",
    "\n",
    "# Â¿Resuelve y conecta a spark-master:7077 y namenode:8020?\n",
    "for host, port in [(\"spark-master\", 7077), (\"namenode\", 8020)]:\n",
    "    try:\n",
    "        s = socket.create_connection((host, port), timeout=3)\n",
    "        print(f\"OK conexiÃ³n a {host}:{port}\")\n",
    "        s.close()\n",
    "    except Exception as e:\n",
    "        print(f\"FALLO conexiÃ³n a {host}:{port} â†’ {e.__class__.__name__}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deac080",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.sparkContext.getConf().getAll())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b571421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"JAVA_HOME:\", os.environ.get(\"JAVA_HOME\"))\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Spark anterior detenido\")\n",
    "except:\n",
    "    print(\"No habÃ­a sesiÃ³n activa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33dca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Validacion-Spark-HDFS-Dary\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.executor.memory\", \"3g\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .getOrCreate())\n",
    "\n",
    "print(\"âœ… Spark version:\", spark.version)\n",
    "print(\"âœ… Master URL:\", spark.sparkContext.master)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ee273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Validacion-Spark-HDFS-Dary\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.executor.memory\", \"3g\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    # ðŸ”‘ Forzar a Spark a usar los XML de Hadoop\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:8020\")\n",
    "    .getOrCreate())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d74df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c425f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"set hive.metastore.uris\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# DataFrame de prueba\n",
    "df = spark.createDataFrame([\n",
    "    Row(id=1, ciudad=\"Manta\", total=185),\n",
    "    Row(id=2, ciudad=\"Lima\",  total=3.0),\n",
    "    Row(id=3, ciudad=\"Quito\", total=4.0),\n",
    "])\n",
    "\n",
    "# Ruta en HDFS\n",
    "out_bronze = \"hdfs://namenode:8020/bronze/data_df/\"\n",
    "\n",
    "# Escritura en HDFS\n",
    "(df.coalesce(1)\n",
    "   .write.mode(\"overwrite\")\n",
    "   .parquet(out_bronze))\n",
    "\n",
    "print(\"âœ… Escrito en HDFS â†’\", out_bronze)\n",
    "\n",
    "# Lectura desde HDFS\n",
    "df2 = spark.read.parquet(out_bronze)\n",
    "print(\"Filas leÃ­das:\", df2.count())\n",
    "df2.orderBy(col(\"total\").desc()).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e7b55ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'Unable to infer schema for Parquet. It must be specified manually.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o456.parquet.\n: org.apache.spark.sql.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:185)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:185)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:184)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:645)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_859/3949359919.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Leer dataset clientes desde HDFS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclientes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://namenode:8020/bronze/clientes/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \"\"\"\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Unable to infer schema for Parquet. It must be specified manually.;'"
     ]
    }
   ],
   "source": [
    "# Leer dataset clientes desde HDFS\n",
    "clientes = spark.read.parquet(\"hdfs://namenode:8020/bronze/clientes/\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5822b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Mostrar primeras filas\n",
    "clientes.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85530647",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mostrar esquema\n",
    "clientes.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, col\n",
    "\n",
    "# Leer Bronze\n",
    "clientes = spark.read.parquet(\"hdfs://namenode:8020/bronze/clientes/\")\n",
    "\n",
    "# Agregar columnas de particiÃ³n (a partir de CreateTime)\n",
    "clientes_part = (clientes\n",
    "    .withColumn(\"year\", year(col(\"CreateTime\")))\n",
    "    .withColumn(\"month\", month(col(\"CreateTime\")))\n",
    "    .withColumn(\"day\", dayofmonth(col(\"CreateTime\")))\n",
    ")\n",
    "\n",
    "# Guardar en Silver particionado\n",
    "out_silver = \"hdfs://namenode:8020/silver/clientes/\"\n",
    "(clientes_part\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\", \"day\")\n",
    "    .parquet(out_silver))\n",
    "\n",
    "print(\"âœ… Clientes guardados en Silver particionados por CreateTime â†’\", out_silver)\n",
    "\n",
    "# Validar\n",
    "spark.read.parquet(out_silver).show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d50483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Verificar si el archivo .env existe\n",
    "env_path = \"/etc/credenciales/.env\"\n",
    "print(f\"ðŸ“ Verificando archivo: {env_path}\")\n",
    "print(f\"ðŸ“ Archivo existe: {os.path.exists(env_path)}\")\n",
    "\n",
    "# Forzar recarga del archivo .env\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "# Debug: Ver todas las variables de entorno que empiecen con DB_\n",
    "print(\"\\nðŸ” Variables de entorno DB_:\")\n",
    "for key, value in os.environ.items():\n",
    "    if key.startswith(\"DB_\"):\n",
    "        print(f\"  {key} = {value}\")\n",
    "\n",
    "# ConfiguraciÃ³n de la base de datos\n",
    "db_config = {\n",
    "    \"jdbc_url\": os.getenv(\"DB_JDBC_URL\"),\n",
    "    \"database\": os.getenv(\"DB_DATABASE\"),\n",
    "    \"driver\": os.getenv(\"DB_DRIVER\"),\n",
    "    \"user\": os.getenv(\"DB_USER\"),\n",
    "    \"password\": os.getenv(\"DB_PASS\"),\n",
    "    \"port\": os.getenv(\"DB_PORT\")\n",
    "}\n",
    "\n",
    "print(\"\\nâœ… Config cargada:\", db_config)\n",
    "\n",
    "# Debug adicional: verificar cada variable individualmente\n",
    "print(\"\\nðŸ”Ž Debug individual:\")\n",
    "variables = [\"DB_JDBC_URL\", \"DB_DATABASE\", \"DB_DRIVER\", \"DB_USER\", \"DB_PASS\", \"DB_PORT\"]\n",
    "for var in variables:\n",
    "    value = os.getenv(var)\n",
    "    print(f\"  {var}: {value} ({'âœ… OK' if value else 'âŒ MISSING'})\")\n",
    "\n",
    "# Leer archivo .env directamente para debug\n",
    "try:\n",
    "    print(f\"\\nðŸ“– Contenido del archivo {env_path}:\")\n",
    "    with open(env_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines, 1):\n",
    "            # Mostrar caracteres especiales\n",
    "            line_repr = repr(line.rstrip())\n",
    "            print(f\"  LÃ­nea {i}: {line_repr}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error leyendo archivo: {e}\")\n",
    "\n",
    "# FunciÃ³n para cargar manualmente si dotenv falla\n",
    "def load_env_manual(file_path):\n",
    "    print(f\"\\nðŸ”§ Intentando carga manual del archivo...\")\n",
    "    env_vars = {}\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                original_line = line\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith('#') and '=' in line:\n",
    "                    try:\n",
    "                        key, value = line.split('=', 1)\n",
    "                        key = key.strip()\n",
    "                        value = value.strip()\n",
    "                        env_vars[key] = value\n",
    "                        os.environ[key] = value\n",
    "                        print(f\"  âœ… Cargada: {key} = {value}\")\n",
    "                    except ValueError as e:\n",
    "                        print(f\"  âŒ Error en lÃ­nea {line_num}: {repr(original_line.strip())} -> {e}\")\n",
    "                elif line:\n",
    "                    print(f\"  âš ï¸  LÃ­nea ignorada {line_num}: {repr(original_line.strip())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error en carga manual: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    return env_vars\n",
    "\n",
    "# Intentar carga manual para las variables faltantes\n",
    "print(\"\\nâš ï¸  Algunas variables faltan, intentando carga manual...\")\n",
    "manual_vars = load_env_manual(env_path)\n",
    "\n",
    "# Verificar de nuevo despuÃ©s de la carga manual\n",
    "print(\"\\nðŸ”„ VerificaciÃ³n despuÃ©s de carga manual:\")\n",
    "for var in variables:\n",
    "    value = os.getenv(var)\n",
    "    print(f\"  {var}: {value} ({'âœ… OK' if value else 'âŒ STILL MISSING'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfbf953",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c8ab181",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j in /opt/spark/python, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1095/2408746234.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    161\u001b[0m             raise Exception(\n\u001b[1;32m    162\u001b[0m                 \"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\".format(\n\u001b[0;32m--> 163\u001b[0;31m                     \u001b[0mspark_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                 )\n\u001b[1;32m    165\u001b[0m             )\n",
      "\u001b[0;31mException\u001b[0m: Unable to find py4j in /opt/spark/python, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Validacion-SQLServer\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.executor.memory\", \"3g\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\")  # ðŸ‘ˆ necesario\n",
    "    .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "152499ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1095/3247168197.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m df = (spark.read.format(\"jdbc\")\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"jdbc_url\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dbtable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dbo.Clientes\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# ðŸ‘ˆ ajusta a tu tabla real\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"password\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"password\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "df = (spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", db_config[\"jdbc_url\"])\n",
    "    .option(\"dbtable\", \"dbo.Clientes\")   # ðŸ‘ˆ ajusta a tu tabla real\n",
    "    .option(\"user\", db_config[\"user\"])\n",
    "    .option(\"password\", db_config[\"password\"])\n",
    "    .option(\"driver\", db_config[\"driver\"])\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc299b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/scripts/config\")   # ruta donde vive db_config.py\n",
    "from db_config import db_config\n",
    "\n",
    "print(db_config[\"jdbc_url\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e95d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Contenido completo de db_config ===\")\n",
    "for key, value in db_config.items():\n",
    "    if key == \"password\":\n",
    "        print(f\"{key}: {'*' * len(str(value))}\")  # Ocultar password\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "jar_path = \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\"\n",
    "print(f\"JAR exists: {os.path.exists(jar_path)}\")\n",
    "\n",
    "if os.path.exists(jar_path):\n",
    "    print(f\"JAR size: {os.path.getsize(jar_path)} bytes\")\n",
    "    print(\"âœ… El JAR estÃ¡ montado correctamente\")\n",
    "else:\n",
    "    print(\"âŒ El JAR no estÃ¡ disponible en el contenedor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70342d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ No habÃ­a sesiÃ³n previa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/10 16:07:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark version: 2.4.5\n",
      "âœ… Master URL: spark://spark-master:7077\n",
      "âœ… Spark reiniciado con driver JDBC\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Cerrar sesiÃ³n actual\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"âœ… SesiÃ³n anterior cerrada\")\n",
    "except:\n",
    "    print(\"â„¹ï¸ No habÃ­a sesiÃ³n previa\")\n",
    "\n",
    "# Reiniciar con configuraciÃ³n explÃ­cita del JAR\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Validacion-Spark-HDFS-Dary\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.executor.memory\", \"3g\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\")\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\")\n",
    "    .config(\"spark.executor.extraClassPath\", \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate())\n",
    "\n",
    "print(\"âœ… Spark version:\", spark.version)\n",
    "print(\"âœ… Master URL:\", spark.sparkContext.master)\n",
    "print(\"âœ… Spark reiniciado con driver JDBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e9ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "jar_path = \"/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar\"\n",
    "print(f\"JAR exists: {os.path.exists(jar_path)}\")\n",
    "\n",
    "if os.path.exists(jar_path):\n",
    "    print(f\"JAR size: {os.path.getsize(jar_path)} bytes\")\n",
    "    print(\"âœ… El JAR estÃ¡ disponible!\")\n",
    "else:\n",
    "    print(\"âŒ El JAR aÃºn no estÃ¡ disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29e9207e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Â¡ConexiÃ³n JDBC exitosa!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|test_col|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importar configuraciÃ³n\n",
    "import sys\n",
    "sys.path.append(\"/scripts/config\")\n",
    "from db_config import db_config\n",
    "\n",
    "# Probar conexiÃ³n\n",
    "try:\n",
    "    df_test = (spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", db_config[\"jdbc_url\"])\n",
    "        .option(\"user\", db_config[\"user\"])\n",
    "        .option(\"password\", db_config[\"password\"])\n",
    "        .option(\"driver\", db_config[\"driver\"])\n",
    "        .option(\"query\", \"SELECT 1 AS test_col\")\n",
    "        .load())\n",
    "    \n",
    "    print(\"âœ… Â¡ConexiÃ³n JDBC exitosa!\")\n",
    "    df_test.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d49e4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Tablas disponibles en la base de datos 'olva':\n",
      "+-------+------------+----------+\n",
      "|esquema|nombre_tabla|tipo_tabla|\n",
      "+-------+------------+----------+\n",
      "|dbo    |Clientes    |BASE TABLE|\n",
      "+-------+------------+----------+\n",
      "\n",
      "\n",
      "ðŸ“ˆ Total de tablas mostradas: 1\n"
     ]
    }
   ],
   "source": [
    "# Consultar las tablas disponibles (versiÃ³n simplificada)\n",
    "try:\n",
    "    df_tablas = (spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", db_config[\"jdbc_url\"])\n",
    "        .option(\"user\", db_config[\"user\"])\n",
    "        .option(\"password\", db_config[\"password\"])\n",
    "        .option(\"driver\", db_config[\"driver\"])\n",
    "        .option(\"query\", \"\"\"\n",
    "            SELECT TOP 100\n",
    "                TABLE_SCHEMA as esquema,\n",
    "                TABLE_NAME as nombre_tabla,\n",
    "                TABLE_TYPE as tipo_tabla\n",
    "            FROM INFORMATION_SCHEMA.TABLES \n",
    "            WHERE TABLE_TYPE = 'BASE TABLE'\n",
    "        \"\"\")\n",
    "        .load())\n",
    "    \n",
    "    print(\"ðŸ“Š Tablas disponibles en la base de datos 'olva':\")\n",
    "    df_tablas.show(100, truncate=False)\n",
    "    print(f\"\\nðŸ“ˆ Total de tablas mostradas: {df_tablas.count()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error consultando tablas: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb329041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sql_query(query: str):\n",
    "    \"\"\"\n",
    "    Ejecuta un query en SQL Server usando la configuraciÃ³n JDBC del proyecto.\n",
    "    Retorna un DataFrame de Spark.\n",
    "    \"\"\"\n",
    "    return (spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", db_config[\"jdbc_url\"])\n",
    "        .option(\"user\", db_config[\"user\"])\n",
    "        .option(\"password\", db_config[\"password\"])\n",
    "        .option(\"driver\", db_config[\"driver\"])\n",
    "        .option(\"query\", query)\n",
    "        .load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6d70761",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clientes = read_sql_query(\"SELECT TOP 10 * FROM dbo.Clientes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "833c5ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------------+----------+--------------------+--------------------+\n",
      "|ClienteID|     Nombre|               Email|  Telefono|          CreateTime|          UpdateTime|\n",
      "+---------+-----------+--------------------+----------+--------------------+--------------------+\n",
      "|        1| Juan PÃ©rez|juan.perez@email.com| 089000100|2025-08-24 21:27:...|2025-08-24 21:31:...|\n",
      "|        2|MarÃ­a LÃ³pez|maria22.nueva@ema...|0987654321|2025-08-24 21:27:...|2025-08-24 21:31:...|\n",
      "|        3|Carlos Ruiz|carlos.ruiz@email...|0971122334|2025-08-24 21:27:...|                null|\n",
      "|        4| Ana Torres|ana.torres@email.com|0965544332|2025-08-24 21:27:...|                null|\n",
      "|        5|Pedro GÃ³mez|pedro.gomez@email...|0956677889|2025-08-24 21:27:...|                null|\n",
      "|        7|  leo PÃ©rez| leo.perez@email.com|0991234522|2025-08-29 19:50:...|                null|\n",
      "+---------+-----------+--------------------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clientes.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a5bdf41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clientes.write.mode(\"overwrite\").parquet(\"hdfs://namenode:8020/bronze/clientes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7caaf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_clientes.write.mode(\"overwrite\").parquet(\"hdfs://namenode:8020/bronze/cliente_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b36c45dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------------+----------+--------------------+--------------------+\n",
      "|ClienteID|     Nombre|               Email|  Telefono|          CreateTime|          UpdateTime|\n",
      "+---------+-----------+--------------------+----------+--------------------+--------------------+\n",
      "|        1| Juan PÃ©rez|juan.perez@email.com| 089000100|2025-08-24 21:27:...|2025-08-24 21:31:...|\n",
      "|        2|MarÃ­a LÃ³pez|maria22.nueva@ema...|0987654321|2025-08-24 21:27:...|2025-08-24 21:31:...|\n",
      "|        3|Carlos Ruiz|carlos.ruiz@email...|0971122334|2025-08-24 21:27:...|                null|\n",
      "|        4| Ana Torres|ana.torres@email.com|0965544332|2025-08-24 21:27:...|                null|\n",
      "|        5|Pedro GÃ³mez|pedro.gomez@email...|0956677889|2025-08-24 21:27:...|                null|\n",
      "+---------+-----------+--------------------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_validacion = spark.read.parquet(\"hdfs://namenode:8020/bronze/clientes\")\n",
    "df_validacion.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6dcf7b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------------------+----------+-----------------------+-----------------------+\n",
      "|ClienteID|Nombre     |Email                  |Telefono  |CreateTime             |UpdateTime             |\n",
      "+---------+-----------+-----------------------+----------+-----------------------+-----------------------+\n",
      "|1        |Juan PÃ©rez |juan.perez@email.com   |089000100 |2025-08-24 21:27:20.383|2025-08-24 21:31:48.536|\n",
      "|2        |MarÃ­a LÃ³pez|maria22.nueva@email.com|0987654321|2025-08-24 21:27:20.383|2025-08-24 21:31:59.697|\n",
      "|3        |Carlos Ruiz|carlos.ruiz@email.com  |0971122334|2025-08-24 21:27:20.383|null                   |\n",
      "|4        |Ana Torres |ana.torres@email.com   |0965544332|2025-08-24 21:27:20.383|null                   |\n",
      "|5        |Pedro GÃ³mez|pedro.gomez@email.com  |0956677889|2025-08-24 21:27:20.383|null                   |\n",
      "+---------+-----------+-----------------------+----------+-----------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max as spark_max\n",
    "df_clientes = spark.read.parquet(\"hdfs://namenode:8020/bronze/clientes\")\n",
    "df_clientes.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6efae1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|  ultima_fecha_carga|\n",
      "+--------------------+\n",
      "|2025-08-29 19:50:...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clientes.agg(spark_max(\"CreateTime\").alias(\"ultima_fecha_carga\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bb1c6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "# Crear DF con el watermark\n",
    "watermark_df = spark.createDataFrame([Row(tabla=\"clientes\",last_watermark=\"2025-08-29 19:50:42\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8acbd0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Watermark inicial guardado en /control/clientes_watermark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Guardar en HDFS\n",
    "watermark_df.write.mode(\"overwrite\").parquet(\"hdfs://namenode:8020/bronze/clientes_watermark\")\n",
    "\n",
    "print(\"âœ… Watermark inicial guardado en /control/clientes_watermark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6e20a125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+\n",
      "|     last_watermark|   tabla|\n",
      "+-------------------+--------+\n",
      "|2025-08-29 19:50:42|clientes|\n",
      "+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ware_df = spark.read.parquet(\"hdfs://namenode:8020/bronze/clientes_watermark\")\n",
    "ware_df.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1cd95256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultima carga de fecha de cliente 2025-08-29 19:50:42\n"
     ]
    }
   ],
   "source": [
    "marca_df= (ware_df\n",
    "           .filter(ware_df.tabla==\"clientes\")\n",
    "           .collect()[0][\"last_watermark\"])\n",
    "print(\"ultima carga de fecha de cliente\",marca_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9b115b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sql_query(query: str):\n",
    "    \"\"\"\n",
    "    Ejecuta un query en SQL Server usando la configuraciÃ³n JDBC del proyecto.\n",
    "    Retorna un DataFrame de Spark.\n",
    "    \"\"\"\n",
    "    return (spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", db_config[\"jdbc_url\"])\n",
    "        .option(\"user\", db_config[\"user\"])\n",
    "        .option(\"password\", db_config[\"password\"])\n",
    "        .option(\"driver\", db_config[\"driver\"])\n",
    "        .option(\"dbtable\", query)   # ðŸ‘ˆ usar dbtable, no query\n",
    "        .load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f4c64aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultima carga de fecha de cliente 2025-08-29 19:50:42\n"
     ]
    }
   ],
   "source": [
    "marca_df= (ware_df\n",
    "           .filter(ware_df.tabla==\"clientes\")\n",
    "           .collect()[0][\"last_watermark\"])\n",
    "print(\"ultima carga de fecha de cliente\",marca_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b29e06ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+----------+-----------------------+----------+\n",
      "|ClienteID|Nombre   |Email              |Telefono  |CreateTime             |UpdateTime|\n",
      "+---------+---------+-------------------+----------+-----------------------+----------+\n",
      "|7        |leo PÃ©rez|leo.perez@email.com|0991234522|2025-08-29 19:50:42.572|null      |\n",
      "|8        |Juan Per |juan.pez@email.com |0991234567|2025-09-10 19:41:11.791|null      |\n",
      "|9        |Maria L  |ma.lopez@email.com |0987654321|2025-09-10 19:41:11.791|null      |\n",
      "+---------+---------+-------------------+----------+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construir query incremental con alias obligatorio\n",
    "query_clientes_incr = f\"(SELECT * FROM dbo.Clientes WHERE CreateTime > '{marca_df}') as clientes_incr\"\n",
    "\n",
    "# Leer con funciÃ³n corregida\n",
    "df_incremental = read_sql_query(query_clientes_incr)\n",
    "df_incremental.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "49fc8f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Datos incrementales guardados en Bronze/clientes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_incremental.write.mode(\"append\").parquet(\"hdfs://namenode:8020/bronze/clientes\")\n",
    "print(\"âœ… Datos incrementales guardados en Bronze/clientes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "324600a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------------------+----------+-----------------------+-----------------------+\n",
      "|ClienteID|Nombre     |Email                  |Telefono  |CreateTime             |UpdateTime             |\n",
      "+---------+-----------+-----------------------+----------+-----------------------+-----------------------+\n",
      "|1        |Juan PÃ©rez |juan.perez@email.com   |089000100 |2025-08-24 21:27:20.383|2025-08-24 21:31:48.536|\n",
      "|2        |MarÃ­a LÃ³pez|maria22.nueva@email.com|0987654321|2025-08-24 21:27:20.383|2025-08-24 21:31:59.697|\n",
      "|3        |Carlos Ruiz|carlos.ruiz@email.com  |0971122334|2025-08-24 21:27:20.383|null                   |\n",
      "|4        |Ana Torres |ana.torres@email.com   |0965544332|2025-08-24 21:27:20.383|null                   |\n",
      "|5        |Pedro GÃ³mez|pedro.gomez@email.com  |0956677889|2025-08-24 21:27:20.383|null                   |\n",
      "|7        |leo PÃ©rez  |leo.perez@email.com    |0991234522|2025-08-29 19:50:42.572|null                   |\n",
      "|7        |leo PÃ©rez  |leo.perez@email.com    |0991234522|2025-08-29 19:50:42.572|null                   |\n",
      "|8        |Juan Per   |juan.pez@email.com     |0991234567|2025-09-10 19:41:11.791|null                   |\n",
      "|9        |Maria L    |ma.lopez@email.com     |0987654321|2025-09-10 19:41:11.791|null                   |\n",
      "+---------+-----------+-----------------------+----------+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clie = spark.read.parquet(\"hdfs://namenode:8020/bronze/clientes\")\n",
    "df_clie.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ec83587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|  ultima_fecha_carga|\n",
      "+--------------------+\n",
      "|2025-09-10 19:41:...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clie.agg(spark_max(\"CreateTime\").alias(\"ultima_fecha_carga\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f814f532",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1095/548886805.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d8f2502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\n",
      "total 4\n",
      "drwsrwsr-x 2 jovyan users 4096 Oct  9  2022 work\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls -l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9cdb2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\DARY\\Desktop\\Dary\\Prueba-BigData\\Proyecto-hive\\scripts\")\n",
    "\n",
    "from parametrossh import init_spark, get_sqlserver_connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f3da491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "sys.path.append(r\"C:\\Users\\DARY\\Desktop\\Dary\\Prueba-BigData\\Proyecto-hive\\scripts\")\n",
    "\n",
    "import parametrossh\n",
    "importlib.reload(parametrossh)\n",
    "\n",
    "from parametrossh import get_params, init_spark, get_sqlserver_connection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5601c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SparkSession inicializada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/25 20:13:18 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/09/25 20:13:33 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/09/25 20:13:48 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/09/25 20:14:03 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "# 1. Iniciar sesiÃ³n de Spark\n",
    "spark = init_spark(1, \"CargaPedidos\")\n",
    "print(\"âœ… SparkSession inicializada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b19ee880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Pedidos desde SQL Server\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+-----------+--------------------+----------+\n",
      "|PedidoID|ClienteID| Monto|FechaPedido|          CreateTime|UpdateTime|\n",
      "+--------+---------+------+-----------+--------------------+----------+\n",
      "|       1|        1|150.50| 2025-09-01|2025-09-14 19:36:...|      null|\n",
      "|       2|        2|200.00| 2025-09-02|2025-09-14 19:36:...|      null|\n",
      "|       3|        3| 50.00| 2025-09-03|2025-09-14 19:36:...|      null|\n",
      "|       4|        4|300.00| 2025-09-04|2025-09-14 19:36:...|      null|\n",
      "|       5|        5|120.00| 2025-09-05|2025-09-14 19:36:...|      null|\n",
      "+--------+---------+------+-----------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/25 20:14:18 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "# 2. ConexiÃ³n SQL Server\n",
    "jdbc_url, properties = get_sqlserver_connection(1)\n",
    "\n",
    "# 3. Leer tabla Pedidos\n",
    "df_pedidos = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"dbo.Pedidos\",\n",
    "    properties=properties\n",
    ")\n",
    "print(\"âœ… Pedidos desde SQL Server\")\n",
    "df_pedidos.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dff9dc2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_pedidos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_442/2577007269.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_pedidos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_pedidos' is not defined"
     ]
    }
   ],
   "source": [
    "df_pedidos.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee703011",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_442/1880794843.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Crear DF con el watermark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwatermark_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtabla\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"clientes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast_watermark\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2025-09-14 19:36:35.439\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# convert python objects to sql data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoInternal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_numpy_record_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mparallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \"\"\"\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mnumSlices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumSlices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnumSlices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mdefaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    422\u001b[0m         reduce tasks)\n\u001b[1;32m    423\u001b[0m         \"\"\"\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "# Crear DF con el watermark\n",
    "watermark_df = spark.createDataFrame([Row(tabla=\"clientes\",last_watermark=\"2025-09-14 19:36:35.439\")])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
