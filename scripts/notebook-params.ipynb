{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208eabde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido de db_configMB:\n",
      "server = mariadb\n",
      "port = 3306\n",
      "database = etl_config\n",
      "user = root\n",
      "password = StrongPass123\n"
     ]
    }
   ],
   "source": [
    "# Importar configuraciÃ³n\n",
    "import sys\n",
    "sys.path.append(\"/scripts/config\")\n",
    "from db_config import db_configMaria\n",
    "\n",
    "print(\"Contenido de db_configMB:\")\n",
    "for key, value in db_configMaria.items():\n",
    "    print(f\"{key} = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dc3e83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ParÃ¡metros SPARK: {'spark.master': 'spark://spark-master:7077', 'spark.executor.instances': '1', 'spark.executor.cores': '1', 'spark.executor.memory': '2g', 'spark.driver.memory': '1g', 'spark.jars': '/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar', 'spark.driver.extraClassPath': '/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar', 'spark.executor.extraClassPath': '/opt/spark/jars/mssql-jdbc-13.2.0.jre8.jar'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/scripts/config\")   # para encontrar db_config\n",
    "sys.path.append(\"/scripts\")          # para encontrar tu archivo parametrossh.py\n",
    "\n",
    "from parametrossh import get_params\n",
    "\n",
    "# Probar con SPARK\n",
    "spark_params = get_params(\"SPARK\", 1)\n",
    "print(\"âœ… ParÃ¡metros SPARK:\", spark_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b28e7741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ParÃ¡metros SQLSERVER: {'sqlserver.host': 'host.docker.internal', 'sqlserver.port': '1433', 'sqlserver.database': 'olva', 'sqlserver.user': 'etl_user', 'sqlserver.password': 'StrongPass123', 'sqlserver.driver': 'com.microsoft.sqlserver.jdbc.SQLServerDriver', 'sqlserver.url': 'jdbc:sqlserver://host.docker.internal:1433;databaseName=olva;encrypt=false;trustServerCertificate=true'}\n"
     ]
    }
   ],
   "source": [
    "# Probar con SQLSERVER\n",
    "sqlserver_params = get_params(\"SQLSERVER\", 1)\n",
    "print(\"âœ… ParÃ¡metros SQLSERVER:\", sqlserver_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f760654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from parametrossh import get_params   # ya la tienes lista\n",
    "\n",
    "def init_spark(ambiente=1, app_name=\"ProyectoHive\"):\n",
    "    # 1. Recuperar parÃ¡metros desde MariaDB\n",
    "    spark_params = get_params(\"SPARK\", ambiente)\n",
    "\n",
    "    # 2. Construir el SparkSession con esos parÃ¡metros\n",
    "    builder = SparkSession.builder.appName(app_name)\n",
    "    for key, value in spark_params.items():\n",
    "        builder = builder.config(key, value)\n",
    "\n",
    "    # 3. Crear sesiÃ³n\n",
    "    spark = builder.getOrCreate()\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8094306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, parametrossh\n",
    "importlib.reload(parametrossh)\n",
    "\n",
    "from parametrossh import init_spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f67834ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SparkSession', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'db_configMaria', 'get_params', 'init_spark', 'mysql', 'pymysql']\n"
     ]
    }
   ],
   "source": [
    "import parametrossh\n",
    "print(dir(parametrossh))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdc143c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/22 20:25:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SparkSession levantada correctamente\n",
      "VersiÃ³n: 2.4.5\n",
      "Master URL: spark://spark-master:7077\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/scripts\")\n",
    "\n",
    "from parametrossh import init_spark\n",
    "\n",
    "spark = init_spark(1, app_name=\"PruebaParametros\")\n",
    "print(\"âœ… SparkSession levantada correctamente\")\n",
    "print(\"VersiÃ³n:\", spark.version)\n",
    "print(\"Master URL:\", spark.sparkContext.master)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ca26522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Rutas HDFS: {'hdfs.bronze': 'hdfs://namenode:8020/bronze', 'hdfs.silver': 'hdfs://namenode:8020/silver', 'hdfs.data': 'hdfs://namenode:8020/data', 'hdfs.tmp': 'hdfs://namenode:8020/tmp', 'hdfs.user': 'hdfs://namenode:8020/user'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/scripts\")\n",
    "\n",
    "import importlib, parametrossh\n",
    "importlib.reload(parametrossh)\n",
    "\n",
    "hdfs_paths = parametrossh.get_hdfs_paths(1)\n",
    "print(\"âœ… Rutas HDFS:\", hdfs_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59c9f30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORDEN=1 | hdfs.bronze = hdfs://namenode:8020/bronze\n",
      "ORDEN=2 | hdfs.silver = hdfs://namenode:8020/silver\n",
      "ORDEN=3 | hdfs.data = hdfs://namenode:8020/data\n",
      "ORDEN=4 | hdfs.tmp = hdfs://namenode:8020/tmp\n",
      "ORDEN=5 | hdfs.user = hdfs://namenode:8020/user\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/scripts\")\n",
    "\n",
    "import importlib, parametrossh\n",
    "importlib.reload(parametrossh)\n",
    "\n",
    "# ahora sÃ­\n",
    "hdfs_params = parametrossh.get_params_full(\"HDFS\", 1)\n",
    "\n",
    "for row in hdfs_params:\n",
    "    print(f\"ORDEN={row['ORDEN']} | {row['PARAMETRO']} = {row['VALOR']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2bc14f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://namenode:8020/bronze\n"
     ]
    }
   ],
   "source": [
    "bronze_path = get_params(\"HDFS\", 1)[\"hdfs.bronze\"]\n",
    "\n",
    "print(bronze_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e397e1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La ruta bronze: hdfs://namenode:8020/bronze\n",
      "\n",
      "Contenido de bronze:\n",
      "==================================================\n",
      "ðŸ“ cliente_df\n",
      "ðŸ“ clientes\n",
      "ðŸ“ clientes_partitioned\n",
      "ðŸ“ clientes_watermark\n",
      "ðŸ“ data_df\n",
      "ðŸ“ df_pedidos\n",
      "ðŸ“ df_pedidos_hive\n",
      "ðŸ“ practica\n",
      "ðŸ“ update_clientes_watermark\n",
      "ðŸ“ validacion_df\n"
     ]
    }
   ],
   "source": [
    "# Ya tienes la ruta\n",
    "bronze_path = get_params(\"HDFS\", 1)[\"hdfs.bronze\"]\n",
    "print(f\"La ruta bronze: {bronze_path}\")\n",
    "\n",
    "# Ahora vemos quÃ© hay dentro - ConfiguraciÃ³n corregida para HDFS\n",
    "hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hdfs_uri = spark.sparkContext._jvm.java.net.URI.create(\"hdfs://namenode:8020\")\n",
    "fs = spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(hdfs_uri, hadoop_conf)\n",
    "path = spark.sparkContext._jvm.org.apache.hadoop.fs.Path(bronze_path)\n",
    "\n",
    "print(f\"\\nContenido de bronze:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "file_status = fs.listStatus(path)\n",
    "for status in file_status:\n",
    "    file_path = status.getPath().toString()\n",
    "    is_dir = status.isDirectory()\n",
    "    name = file_path.split('/')[-1]\n",
    "    \n",
    "    if is_dir:\n",
    "        print(f\"ðŸ“ {name}\")\n",
    "    else:\n",
    "        size = status.getLen()\n",
    "        print(f\"ðŸ“„ {name} ({size} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae66069f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trabajando con: hdfs://namenode:8020/bronze/df_pedidos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Ya tienes la ruta base de bronze\n",
    "bronze_path = get_params(\"HDFS\", 1)[\"hdfs.bronze\"]\n",
    "\n",
    "# Elegir una tabla para trabajar (cambia por la que necesites)\n",
    "tabla_elegida = \"df_pedidos\"  # o \"cliente_df\", \"clientes\", etc.\n",
    "\n",
    "# Construir la ruta completa\n",
    "ruta_tabla = f\"{bronze_path}/{tabla_elegida}\"\n",
    "print(f\"Trabajando con: {ruta_tabla}\")\n",
    "\n",
    "# Leer la tabla\n",
    "df = spark.read.parquet(ruta_tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98c513bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+-----------+--------------------+----------+\n",
      "|PedidoID|ClienteID| Monto|FechaPedido|          CreateTime|UpdateTime|\n",
      "+--------+---------+------+-----------+--------------------+----------+\n",
      "|       1|        1|150.50| 2025-09-01|2025-09-14 19:36:...|      null|\n",
      "|       2|        2|200.00| 2025-09-02|2025-09-14 19:36:...|      null|\n",
      "|       3|        3| 50.00| 2025-09-03|2025-09-14 19:36:...|      null|\n",
      "|       4|        4|300.00| 2025-09-04|2025-09-14 19:36:...|      null|\n",
      "|       5|        5|120.00| 2025-09-05|2025-09-14 19:36:...|      null|\n",
      "|       6|        1| 80.00| 2025-09-06|2025-09-14 19:36:...|      null|\n",
      "|       7|        2| 60.00| 2025-09-07|2025-09-14 19:36:...|      null|\n",
      "|       8|        3| 90.00| 2025-09-08|2025-09-14 19:36:...|      null|\n",
      "|       9|        4|110.00| 2025-09-09|2025-09-14 19:36:...|      null|\n",
      "|      10|        5| 75.00| 2025-09-10|2025-09-14 19:36:...|      null|\n",
      "+--------+---------+------+-----------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
