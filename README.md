ğŸ“Š TFM-Proyecto-Hadoop

Proyecto de anÃ¡lisis de datos con arquitectura Big Data basada en Hadoop, Spark y Hive, que permite la extracciÃ³n, procesamiento, limpieza y almacenamiento
de datos desde distintas fuentes hacia un Data Lake (HDFS) y su exposiciÃ³n final en MySQL para anÃ¡lisis y visualizaciÃ³n.

ğŸ“– Ãndice

ğŸ”§ Requisitos previos

ğŸ§± Arquitectura del proyecto

ğŸ“‚ Estructura del repositorio

âš™ï¸ InstalaciÃ³n y configuraciÃ³n

ğŸš€ EjecuciÃ³n de procesos ETL

ğŸ§  Procesamiento en Spark

ğŸ’¾ Persistencia y consultas con Hive y MySQL

ğŸ“ˆ Buenas prÃ¡cticas y optimizaciÃ³n

ğŸ§° Troubleshooting (problemas comunes)

ğŸ”§ Requisitos previos

Antes de ejecutar el proyecto asegÃºrate de contar con:

Componente	VersiÃ³n recomendada	DescripciÃ³n

âœ… Docker & Docker Compose	â‰¥ 20.10	Contenedores del ecosistema Hadoop

âœ… Python	â‰¥ 3.10	Scripts de control y pruebas locales

âœ… Apache Spark	3.5.x	Motor de procesamiento distribuido

âœ… Apache Hive	3.x	Metastore y capa SQL sobre HDFS

âœ… HDFS (Hadoop)	3.x	Sistema distribuido de archivos

âœ… MySQL Server	â‰¥ 8.x	Base de datos relacional para resultados

âœ… Git	-	Control de versiones del proyecto

ğŸ“‚ Estructura del proyecto

TFM-Proyecto-Hadoop/

â”‚â”€â”€ docker-compose.yml         # Levanta HDFS, Spark y Hive

â”‚â”€â”€ spark/                     # ConfiguraciÃ³n de Spark y scripts ETL

â”‚   â”œâ”€â”€ etl_bronze_pedidos.py  # ExtracciÃ³n y carga en capa Bronze

â”‚   â”œâ”€â”€ etl_silver_clientes.py # Limpieza y enriquecimiento

â”‚   â”œâ”€â”€ etl_gold_reportes.py   # Agregaciones finales

â”‚   â”œâ”€â”€ create_bronze_tables.sql
â”‚   â”œâ”€â”€ create_silver_tables.sql
â”‚â”€â”€ mysql/                     # Exportadores y conexiÃ³n a MySQL

â”‚â”€â”€ notebooks/                 # Notebooks de anÃ¡lisis exploratorio

â”‚â”€â”€ data/                      # Datos fuente (si aplica)

â”‚â”€â”€ requirements.txt           # Dependencias Python

â”‚â”€â”€ README.md                  # DocumentaciÃ³n del proyecto

â”‚â”€â”€ .env                       # Variables de entorno del cluster



âš™ï¸ InstalaciÃ³n y configuraciÃ³n

1ï¸âƒ£ Clona el repositorio

git clone https://github.com/alexander2295/TFM-Proyecto-Hadoop.git
cd TFM-Proyecto-Hadoop


2ï¸âƒ£ Levanta el ecosistema Hadoop

docker-compose up -d


Esto iniciarÃ¡:

namenode, datanode â†’ HDFS

spark-master, spark-worker â†’ Spark Cluster

hive-metastore, hive-server â†’ CatÃ¡logo SQL

mysql â†’ Persistencia final

3ï¸âƒ£ Verifica que los servicios estÃ©n activos

docker ps


4ï¸âƒ£ Instala dependencias locales

pip install -r requirements.txt


